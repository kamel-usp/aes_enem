{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inference Validation Notebook\n",
    "\n",
    "TL;DR - \n",
    "\n",
    "This notebook is a general purposes that serves to run [trained models](https://huggingface.co/kamel-usp) so we can validate reported results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Seed set to 42\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'kamel-usp/aes_enem_models-sourceA-ordinal-from-bertimbau-base-C1'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "from datasets import load_dataset, DatasetDict, Dataset\n",
    "from transformers import AutoModelForSequenceClassification, AutoTokenizer\n",
    "from sklearn.metrics import accuracy_score, cohen_kappa_score, mean_squared_error\n",
    "import pytorch_lightning as pl\n",
    "import torch\n",
    "from tqdm.auto import tqdm\n",
    "import pandas as pd\n",
    "from coral_pytorch.dataset import corn_label_from_logits\n",
    "\n",
    "\n",
    "RANDOM_SEED = 42\n",
    "REFERENCE_CONCEPT = 0\n",
    "OBJECTIVE = \"ordinal\"\n",
    "MAX_LENGTH = 512\n",
    "BATCH_SIZE=16\n",
    "VARIANT = \"base\" #base/large\n",
    "TOKENIZER_NAME = f\"neuralmind/bert-{VARIANT}-portuguese-cased\"\n",
    "BASE_MODEL = \"bertimbau\" #bertimbau/sourceB-mlm/sourceB-ordinal\n",
    "MODEL_NAME = f\"kamel-usp/aes_enem_models-sourceA-ordinal-from-{BASE_MODEL}-{VARIANT}-C{REFERENCE_CONCEPT+1}\"\n",
    "\n",
    "pl.seed_everything(RANDOM_SEED)\n",
    "torch.set_float32_matmul_precision('medium')\n",
    "MODEL_NAME"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = load_dataset(\"kamel-usp/aes_enem_dataset\", \"sourceAWithGraders\", cache_dir=\"/tmp/aes_enem\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['id', 'id_prompt', 'essay_title', 'essay_text', 'grades', 'essay_year'],\n",
       "        num_rows: 744\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['id', 'id_prompt', 'essay_title', 'essay_text', 'grades', 'essay_year'],\n",
       "        num_rows: 195\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['id', 'id_prompt', 'essay_title', 'essay_text', 'grades', 'essay_year'],\n",
       "        num_rows: 216\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "grade_mapping = {\n",
    "    0: 0,\n",
    "    40: 1,\n",
    "    80: 2,\n",
    "    120: 3,\n",
    "    160: 4,\n",
    "    200: 5,\n",
    "}\n",
    "\n",
    "def create_label(row):\n",
    "    grade = row[\"grades\"][REFERENCE_CONCEPT]\n",
    "    return {\"label\": grade_mapping[grade]}\n",
    "\n",
    "dataset = dataset.map(create_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_difference(lists):\n",
    "    # Assuming the first element is the reference for subtraction\n",
    "    reference = lists[0][REFERENCE_CONCEPT]\n",
    "    grader_a = lists[1][REFERENCE_CONCEPT]\n",
    "    grader_b = lists[2][REFERENCE_CONCEPT]\n",
    "\n",
    "    # Calculate absolute differences\n",
    "    diff_ref_a = abs(reference - grader_a)\n",
    "    diff_ref_b = abs(reference - grader_b)\n",
    "    diff_a_b = abs(grader_a - grader_b)\n",
    "\n",
    "    # Check if any difference is greater than 80\n",
    "    return diff_ref_a > 80 or diff_ref_b > 80 or diff_a_b > 80\n",
    "\n",
    "test_df = dataset[\"test\"].to_pandas()\n",
    "new_test_df = pd.merge(\n",
    "    test_df.groupby([\"id_prompt\", \"id\"]).agg({\"grades\": list}).apply(lambda x: compute_difference(x['grades']), axis=1).reset_index(),\n",
    "    test_df,\n",
    "    on=[\"id_prompt\",\"id\"]\n",
    ").rename(columns={0: \"is_hard\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset[\"test_easy\"] = Dataset.from_pandas(new_test_df[new_test_df[\"is_hard\"]==False])\n",
    "dataset[\"test_hard\"] = Dataset.from_pandas(new_test_df[new_test_df[\"is_hard\"]==True])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(TOKENIZER_NAME, use_fast=True)\n",
    "def get_model_instance(model_path, objective):\n",
    "    model = None\n",
    "    if objective == \"regression\":\n",
    "        model = AutoModelForSequenceClassification.from_pretrained(\n",
    "                model_path, \n",
    "                cache_dir=\"/tmp/\", \n",
    "                num_labels=1,\n",
    "            )\n",
    "    elif objective == \"classification\" or objective == \"ordinal\":\n",
    "        model = AutoModelForSequenceClassification.from_pretrained(\n",
    "                model_path, \n",
    "                cache_dir=\"/tmp/\", \n",
    "                num_labels=6,\n",
    "            )\n",
    "    return model\n",
    "model = get_model_instance(MODEL_NAME, OBJECTIVE)\n",
    "if model is None:\n",
    "    raise ValueError(\"Please set a Pre defined Objective\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_dataset(dataset):\n",
    "    def tokenize_essays(dataset, tokenizer, max_length=512):\n",
    "        tokenized_text = tokenizer(\n",
    "                dataset[\"essay_text\"],\n",
    "                return_tensors=\"pt\",\n",
    "                truncation=True,\n",
    "                padding=\"max_length\",\n",
    "                max_length=max_length\n",
    "            )\n",
    "        tokenized_text[\"label\"] = dataset[\"label\"]\n",
    "        return tokenized_text\n",
    "    \n",
    "    tokenized_datasets = {\n",
    "        split: tokenize_essays(sub_dataset, tokenizer, MAX_LENGTH)\n",
    "        for split, sub_dataset in dataset.items()\n",
    "    }\n",
    "    dataset_tokenized = DatasetDict({\n",
    "        split: Dataset.from_dict(data)\n",
    "        for split, data in tokenized_datasets.items()\n",
    "    })\n",
    "\n",
    "    return dataset_tokenized\n",
    "\n",
    "dataset_tokenized = prepare_dataset(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_train = DataLoader(\n",
    "    dataset_tokenized[\"train\"].with_format(\"torch\"), batch_size=BATCH_SIZE, shuffle=True, num_workers=0\n",
    ")\n",
    "data_val = DataLoader(dataset_tokenized[\"validation\"].with_format(\"torch\"), batch_size=BATCH_SIZE, shuffle=False, num_workers=0)\n",
    "data_test = DataLoader(dataset_tokenized[\"test\"].with_format(\"torch\"), batch_size=BATCH_SIZE, shuffle=False, num_workers=0)\n",
    "\n",
    "data_test_easy = DataLoader(dataset_tokenized[\"test_easy\"].with_format(\"torch\"), batch_size=BATCH_SIZE, shuffle=False, num_workers=0)\n",
    "data_test_hard = DataLoader(dataset_tokenized[\"test_hard\"].with_format(\"torch\"), batch_size=BATCH_SIZE, shuffle=False, num_workers=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_classes(output):\n",
    "    if OBJECTIVE == \"regression\":\n",
    "        # Round the tensor to the nearest integer\n",
    "        rounded_tensor = torch.round(output.logits)\n",
    "        # Clamp the values to the range [0, 5]\n",
    "        clamped_tensor = torch.clamp(rounded_tensor, min=0, max=5)\n",
    "        return clamped_tensor.view(-1)\n",
    "    elif OBJECTIVE == \"classification\":\n",
    "        return torch.argmax(output.logits, axis=1)\n",
    "    elif OBJECTIVE == \"ordinal\":\n",
    "        return corn_label_from_logits(output.logits)\n",
    "        \n",
    "def get_predictions_and_labels(model, dataloader):\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model.to(device)\n",
    "    all_predictions = []\n",
    "    all_true_labels = []\n",
    "    i=0\n",
    "    for batch in tqdm(dataloader, desc=\"Obtaining predictions\"):\n",
    "        input_ids = batch[\"input_ids\"].to(device)\n",
    "        attention_mask = batch[\"attention_mask\"].to(device)\n",
    "        labels = batch[\"label\"].to(device)\n",
    "        with torch.no_grad():\n",
    "            output = model(input_ids, attention_mask)\n",
    "            predicted_classes = predict_classes(output) \n",
    "\n",
    "        # If using GPU, need to move the data back to CPU to use numpy.\n",
    "        all_predictions.extend(predicted_classes.cpu().numpy())\n",
    "        all_true_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "    return list(map(lambda x: x * 40, all_predictions)), list(map(lambda x: x * 40, all_true_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def enem_accuracy_score(true_values, predicted_values):\n",
    "    assert len(true_values) == len(predicted_values), \"Mismatched length between true and predicted values.\"\n",
    "\n",
    "    non_divergent_count = sum([1 for t, p in zip(true_values, predicted_values) if abs(t - p) <= 80])\n",
    "    \n",
    "    return non_divergent_count / len(true_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d0d8b11230d544b69d78a75ae24a07c0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Obtaining predictions:   0%|          | 0/14 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on the validation  set: 0.49\n"
     ]
    }
   ],
   "source": [
    "all_predictions, all_true_labels = get_predictions_and_labels(model, data_test)\n",
    "accuracy = accuracy_score(all_true_labels, all_predictions)\n",
    "print(f\"Accuracy on the validation  set: {accuracy:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "QWK on the validation set: 0.30\n"
     ]
    }
   ],
   "source": [
    "qwk = cohen_kappa_score(all_true_labels, all_predictions, weights=\"quadratic\", labels=[0,40,80,120,160,200])\n",
    "print(f\"QWK on the validation set: {qwk:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on the validation set: 0.93\n"
     ]
    }
   ],
   "source": [
    "enem_accuracy = enem_accuracy_score(all_true_labels, all_predictions)\n",
    "print(f\"Accuracy on the validation set: {enem_accuracy:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metrics(model, dataset, test_group):\n",
    "    all_predictions, all_true_labels = get_predictions_and_labels(model, dataset)\n",
    "    accuracy = accuracy_score(all_true_labels, all_predictions)\n",
    "    qwk = cohen_kappa_score(all_true_labels, all_predictions, weights=\"quadratic\", labels=[0,40,80,120,160,200]) \n",
    "    rmse = mean_squared_error(all_true_labels, all_predictions, squared=False)\n",
    "    horizontal_discrepancy = enem_accuracy_score(all_true_labels, all_predictions)\n",
    "    result = {\n",
    "        'Experiment Reference': MODEL_NAME,\n",
    "        'Test Group': test_group,\n",
    "        'Competence': REFERENCE_CONCEPT,\n",
    "        'Accuracy': [accuracy],\n",
    "        'RMSE': [rmse],\n",
    "        'QWK': [qwk],\n",
    "        'HDIV': [1- horizontal_discrepancy]\n",
    "    }\n",
    "    return pd.DataFrame(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6084f715ecaa469abc0b2e94f4576517",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Obtaining predictions:   0%|          | 0/14 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Experiment Reference</th>\n",
       "      <th>Test Group</th>\n",
       "      <th>Competence</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>RMSE</th>\n",
       "      <th>QWK</th>\n",
       "      <th>HDIV</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>kamel-usp/aes_enem_models-sourceA-ordinal-from...</td>\n",
       "      <td>full</td>\n",
       "      <td>0</td>\n",
       "      <td>0.486111</td>\n",
       "      <td>44.472214</td>\n",
       "      <td>0.295411</td>\n",
       "      <td>0.074074</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                Experiment Reference Test Group  Competence  \\\n",
       "0  kamel-usp/aes_enem_models-sourceA-ordinal-from...       full           0   \n",
       "\n",
       "   Accuracy       RMSE       QWK      HDIV  \n",
       "0  0.486111  44.472214  0.295411  0.074074  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3a79808aad4746f5b629cf0089db682e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Obtaining predictions:   0%|          | 0/11 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Experiment Reference</th>\n",
       "      <th>Test Group</th>\n",
       "      <th>Competence</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>RMSE</th>\n",
       "      <th>QWK</th>\n",
       "      <th>HDIV</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>kamel-usp/aes_enem_models-sourceA-ordinal-from...</td>\n",
       "      <td>easy</td>\n",
       "      <td>0</td>\n",
       "      <td>0.541667</td>\n",
       "      <td>31.622777</td>\n",
       "      <td>0.427347</td>\n",
       "      <td>0.011905</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                Experiment Reference Test Group  Competence  \\\n",
       "0  kamel-usp/aes_enem_models-sourceA-ordinal-from...       easy           0   \n",
       "\n",
       "   Accuracy       RMSE       QWK      HDIV  \n",
       "0  0.541667  31.622777  0.427347  0.011905  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model.eval()\n",
    "display(compute_metrics(model, data_test, \"full\"))\n",
    "display(compute_metrics(model, data_test_easy, \"easy\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'kamel-usp/aes_enem_models-sourceA-ordinal-from-bertimbau-base-C1'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "MODEL_NAME"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "aes_enem",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
