{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inference Validation Notebook\n",
    "\n",
    "TL;DR - \n",
    "\n",
    "This notebook is a general purposes that serves to run [trained models](https://huggingface.co/kamel-usp) so we can validate reported results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'kamel-usp/aes_enem_models-sourceA-ordinal-from-bertimbau-base-C1'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "from datasets import load_dataset, DatasetDict, Dataset\n",
    "from transformers import AutoModelForSequenceClassification, AutoTokenizer\n",
    "from sklearn.metrics import accuracy_score, cohen_kappa_score, mean_squared_error\n",
    "import torch\n",
    "from tqdm.auto import tqdm\n",
    "import numpy as np\n",
    "import random\n",
    "import pandas as pd\n",
    "from coral_pytorch.dataset import corn_label_from_logits\n",
    "torch.use_deterministic_algorithms(True)\n",
    "\n",
    "RANDOM_SEED = 42\n",
    "REFERENCE_CONCEPT = 0\n",
    "OBJECTIVE = \"ordinal\"\n",
    "MAX_LENGTH = 512\n",
    "BATCH_SIZE=16\n",
    "VARIANT = \"base\" #base/large\n",
    "TOKENIZER_NAME = f\"neuralmind/bert-{VARIANT}-portuguese-cased\"\n",
    "BASE_MODEL = \"bertimbau\" #bertimbau/sourceB-mlm/sourceB-ordinal\n",
    "MODEL_NAME = f\"kamel-usp/aes_enem_models-sourceA-ordinal-from-{BASE_MODEL}-{VARIANT}-C{REFERENCE_CONCEPT+1}\"\n",
    "torch.manual_seed(RANDOM_SEED)\n",
    "\n",
    "MODEL_NAME"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ae347415c10940be95188219c8bb64ea",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading builder script:   0%|          | 0.00/25.4k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e690b000132e4605bd5d46d63eecc42a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data files:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d3ce77a089774fca9c2c6975bcfa3179",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Extracting data files:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d018dfd600af422e877987912f2410cd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Parsing HTML files from: sourceAWithGraders:   0%|          | 0/44 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "092e99b83322446fb86cdc8947187192",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c039f1670a29499bb950a8d006639fb4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating validation split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1ab596e282e746b785b90a814e97e371",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating test split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dataset = load_dataset(\"kamel-usp/aes_enem_dataset\", \"sourceAWithGraders\", cache_dir=\"/tmp/aes_enem\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['id', 'id_prompt', 'essay_title', 'essay_text', 'grades', 'essay_year'],\n",
       "        num_rows: 738\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['id', 'id_prompt', 'essay_title', 'essay_text', 'grades', 'essay_year'],\n",
       "        num_rows: 204\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['id', 'id_prompt', 'essay_title', 'essay_text', 'grades', 'essay_year'],\n",
       "        num_rows: 213\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'id': '5.html',\n",
       " 'id_prompt': 'o-brasil-paralisado-o-que-voce-pensa-sobre-a-greve-dos-caminhoneiros',\n",
       " 'essay_title': 'O Brasil e seus conflitos',\n",
       " 'essay_text': 'É de conhecimento geral que a notícia se espalha pelo Brasil inteiro, pois a manifestação está sendo falada em todo o território nacional. Com base nos conhecimentos os caminhoneiros estão reivindicando seus direitos. Em consequência disso, vê-se a todo instante reportagens que vem a julgar a manifestação, no entanto, dizem que os caminhoneiros estao causando desordem entre a população, nota-se qeos grevistas não estão apenas manifestando para si, e sim àqueles que não tiveram coragem de sair nas ruas a declarar sua indignação. Em todos os canais há notas de esclarecimento que devido a greve, está faltando remédios em hospitais, doadores, etc. Isso é praticamente um meio de não culpar os governantes e querem pôr culpa nos caminhoneiros. Em virtudes dos fatos mencionados, conclui-se que há uma esperança de vida melhor assim que a greve acabar.',\n",
       " 'grades': [0, 0, 0, 0, 0, 0],\n",
       " 'essay_year': 2018}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[\"test\"][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a309c2863a404557a09c8673906748ee",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/738 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "20eaf11937e74bb3a125167cef7fb1f5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/204 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "247de59cb0784560a654bc0bab941305",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/213 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "grade_mapping = {\n",
    "    0: 0,\n",
    "    40: 1,\n",
    "    80: 2,\n",
    "    120: 3,\n",
    "    160: 4,\n",
    "    200: 5,\n",
    "}\n",
    "\n",
    "def create_label(row):\n",
    "    grade = row[\"grades\"][REFERENCE_CONCEPT]\n",
    "    return {\"label\": grade_mapping[grade]}\n",
    "\n",
    "dataset = dataset.map(create_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_difference(lists):\n",
    "    # Assuming the first element is the reference for subtraction\n",
    "    reference = lists[0][REFERENCE_CONCEPT]\n",
    "    grader_a = lists[1][REFERENCE_CONCEPT]\n",
    "    grader_b = lists[2][REFERENCE_CONCEPT]\n",
    "\n",
    "    # Calculate absolute differences\n",
    "    diff_ref_a = abs(reference - grader_a)\n",
    "    diff_ref_b = abs(reference - grader_b)\n",
    "    diff_a_b = abs(grader_a - grader_b)\n",
    "\n",
    "    # Check if any difference is greater than 80\n",
    "    return diff_ref_a > 80 or diff_ref_b > 80 or diff_a_b > 80\n",
    "\n",
    "test_df = dataset[\"test\"].to_pandas()\n",
    "new_test_df = pd.merge(\n",
    "    test_df.groupby([\"id_prompt\", \"id\"]).agg({\"grades\": list}).apply(lambda x: compute_difference(x['grades']), axis=1).reset_index(),\n",
    "    test_df,\n",
    "    on=[\"id_prompt\",\"id\"]\n",
    ").rename(columns={0: \"is_hard\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset[\"test_easy\"] = Dataset.from_pandas(new_test_df[new_test_df[\"is_hard\"]==False])\n",
    "dataset[\"test_hard\"] = Dataset.from_pandas(new_test_df[new_test_df[\"is_hard\"]==True])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(TOKENIZER_NAME, use_fast=True)\n",
    "def get_model_instance(model_path, objective):\n",
    "    model = None\n",
    "    if objective == \"regression\":\n",
    "        model = AutoModelForSequenceClassification.from_pretrained(\n",
    "                model_path, \n",
    "                cache_dir=\"/tmp/\", \n",
    "                num_labels=1,\n",
    "            )\n",
    "    elif objective == \"classification\" or objective == \"ordinal\":\n",
    "        model = AutoModelForSequenceClassification.from_pretrained(\n",
    "                model_path, \n",
    "                cache_dir=\"/tmp/aes_enem2\", \n",
    "                num_labels=6,\n",
    "            )\n",
    "    return model\n",
    "model = get_model_instance(MODEL_NAME, OBJECTIVE)\n",
    "if model is None:\n",
    "    raise ValueError(\"Please set a Pre defined Objective\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_dataset(dataset):\n",
    "    def tokenize_essays(dataset, tokenizer, max_length=512):\n",
    "        tokenized_text = tokenizer(\n",
    "                dataset[\"essay_text\"],\n",
    "                return_tensors=\"pt\",\n",
    "                truncation=True,\n",
    "                padding=\"max_length\",\n",
    "                max_length=max_length\n",
    "            )\n",
    "        tokenized_text[\"label\"] = dataset[\"label\"]\n",
    "        return tokenized_text\n",
    "    \n",
    "    tokenized_datasets = {\n",
    "        split: tokenize_essays(sub_dataset, tokenizer, MAX_LENGTH)\n",
    "        for split, sub_dataset in dataset.items()\n",
    "    }\n",
    "    dataset_tokenized = DatasetDict({\n",
    "        split: Dataset.from_dict(data)\n",
    "        for split, data in tokenized_datasets.items()\n",
    "    })\n",
    "\n",
    "    return dataset_tokenized\n",
    "\n",
    "dataset_tokenized = prepare_dataset(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def seed_worker(worker_id):\n",
    "    worker_seed = torch.initial_seed() % 2**32\n",
    "    np.random.seed(worker_seed)\n",
    "    random.seed(worker_seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "g = torch.Generator()\n",
    "g.manual_seed(RANDOM_SEED)\n",
    "\n",
    "data_train = DataLoader(\n",
    "    dataset_tokenized[\"train\"].with_format(\"torch\"), batch_size=BATCH_SIZE, shuffle=True, num_workers=0,\n",
    "    worker_init_fn=seed_worker,\n",
    "    generator=g\n",
    ")\n",
    "data_val = DataLoader(dataset_tokenized[\"validation\"].with_format(\"torch\"), batch_size=BATCH_SIZE,\n",
    "                       shuffle=False, num_workers=0,worker_init_fn=seed_worker,generator=g)\n",
    "data_test = DataLoader(dataset_tokenized[\"test\"].with_format(\"torch\"), batch_size=BATCH_SIZE,\n",
    "                       shuffle=False, num_workers=0,worker_init_fn=seed_worker,generator=g)\n",
    "\n",
    "data_test_easy = DataLoader(dataset_tokenized[\"test_easy\"].with_format(\"torch\"), batch_size=BATCH_SIZE,\n",
    "                            shuffle=False, num_workers=0,worker_init_fn=seed_worker,generator=g)\n",
    "data_test_hard = DataLoader(dataset_tokenized[\"test_hard\"].with_format(\"torch\"), batch_size=BATCH_SIZE,\n",
    "                            shuffle=False, num_workers=0,worker_init_fn=seed_worker,generator=g)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_classes(output):\n",
    "    if OBJECTIVE == \"regression\":\n",
    "        # Round the tensor to the nearest integer\n",
    "        rounded_tensor = torch.round(output.logits)\n",
    "        # Clamp the values to the range [0, 5]\n",
    "        clamped_tensor = torch.clamp(rounded_tensor, min=0, max=5)\n",
    "        return clamped_tensor.view(-1)\n",
    "    elif OBJECTIVE == \"classification\":\n",
    "        return torch.argmax(output.logits, axis=1)\n",
    "    elif OBJECTIVE == \"ordinal\":\n",
    "        return corn_label_from_logits(output.logits)\n",
    "        \n",
    "def get_predictions_and_labels(model, dataloader):\n",
    "    # device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    device = \"cpu\"\n",
    "    model.to(device)\n",
    "    all_predictions = []\n",
    "    all_true_labels = []\n",
    "    i=0\n",
    "    for batch in tqdm(dataloader, desc=\"Obtaining predictions\"):\n",
    "        input_ids = batch[\"input_ids\"].to(device)\n",
    "        attention_mask = batch[\"attention_mask\"].to(device)\n",
    "        labels = batch[\"label\"].to(device)\n",
    "        with torch.no_grad():\n",
    "            output = model(input_ids, attention_mask)\n",
    "            predicted_classes = predict_classes(output) \n",
    "\n",
    "        # If using GPU, need to move the data back to CPU to use numpy.\n",
    "        all_predictions.extend(predicted_classes.cpu().numpy())\n",
    "        all_true_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "    return list(map(lambda x: x * 40, all_predictions)), list(map(lambda x: x * 40, all_true_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def enem_accuracy_score(true_values, predicted_values):\n",
    "    assert len(true_values) == len(predicted_values), \"Mismatched length between true and predicted values.\"\n",
    "\n",
    "    non_divergent_count = sum([1 for t, p in zip(true_values, predicted_values) if abs(t - p) <= 80])\n",
    "    \n",
    "    return non_divergent_count / len(true_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1b826cc7646b4f159579529f0d70d10c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Obtaining predictions:   0%|          | 0/14 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on the validation  set: 0.53\n"
     ]
    }
   ],
   "source": [
    "all_predictions, all_true_labels = get_predictions_and_labels(model, data_test)\n",
    "accuracy = accuracy_score(all_true_labels, all_predictions)\n",
    "print(f\"Accuracy on the validation  set: {accuracy:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "QWK on the validation set: 0.46\n"
     ]
    }
   ],
   "source": [
    "qwk = cohen_kappa_score(all_true_labels, all_predictions, weights=\"quadratic\", labels=[0,40,80,120,160,200])\n",
    "print(f\"QWK on the validation set: {qwk:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on the validation set: 0.94\n"
     ]
    }
   ],
   "source": [
    "enem_accuracy = enem_accuracy_score(all_true_labels, all_predictions)\n",
    "print(f\"Accuracy on the validation set: {enem_accuracy:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metrics(model, dataset, test_group):\n",
    "    all_predictions, all_true_labels = get_predictions_and_labels(model, dataset)\n",
    "    accuracy = accuracy_score(all_true_labels, all_predictions)\n",
    "    qwk = cohen_kappa_score(all_true_labels, all_predictions, weights=\"quadratic\", labels=[0,40,80,120,160,200]) \n",
    "    rmse = mean_squared_error(all_true_labels, all_predictions, squared=False)\n",
    "    horizontal_discrepancy = enem_accuracy_score(all_true_labels, all_predictions)\n",
    "    result = {\n",
    "        'Experiment Reference': MODEL_NAME,\n",
    "        'Test Group': test_group,\n",
    "        'Competence': REFERENCE_CONCEPT,\n",
    "        'Accuracy': [accuracy],\n",
    "        'RMSE': [rmse],\n",
    "        'QWK': [qwk],\n",
    "        'HDIV': [1- horizontal_discrepancy]\n",
    "    }\n",
    "    return pd.DataFrame(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7cf7cb7b50184029902a170947866843",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Obtaining predictions:   0%|          | 0/14 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Experiment Reference</th>\n",
       "      <th>Test Group</th>\n",
       "      <th>Competence</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>RMSE</th>\n",
       "      <th>QWK</th>\n",
       "      <th>HDIV</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>kamel-usp/aes_enem_models-sourceA-ordinal-from...</td>\n",
       "      <td>full</td>\n",
       "      <td>0</td>\n",
       "      <td>0.525822</td>\n",
       "      <td>43.074343</td>\n",
       "      <td>0.463946</td>\n",
       "      <td>0.056338</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                Experiment Reference Test Group  Competence  \\\n",
       "0  kamel-usp/aes_enem_models-sourceA-ordinal-from...       full           0   \n",
       "\n",
       "   Accuracy       RMSE       QWK      HDIV  \n",
       "0  0.525822  43.074343  0.463946  0.056338  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "88eb5361c8644205913f62234f2d4a42",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Obtaining predictions:   0%|          | 0/11 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Experiment Reference</th>\n",
       "      <th>Test Group</th>\n",
       "      <th>Competence</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>RMSE</th>\n",
       "      <th>QWK</th>\n",
       "      <th>HDIV</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>kamel-usp/aes_enem_models-sourceA-ordinal-from...</td>\n",
       "      <td>easy</td>\n",
       "      <td>0</td>\n",
       "      <td>0.574713</td>\n",
       "      <td>29.556103</td>\n",
       "      <td>0.552396</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                Experiment Reference Test Group  Competence  \\\n",
       "0  kamel-usp/aes_enem_models-sourceA-ordinal-from...       easy           0   \n",
       "\n",
       "   Accuracy       RMSE       QWK  HDIV  \n",
       "0  0.574713  29.556103  0.552396   0.0  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    display(compute_metrics(model, data_test, \"full\"))\n",
    "    display(compute_metrics(model, data_test_easy, \"easy\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'kamel-usp/aes_enem_models-sourceA-ordinal-from-bertimbau-base-C1'"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "MODEL_NAME"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "aes_enem",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
